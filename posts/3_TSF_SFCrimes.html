<!DOCTYPE html>
<html lang="en"> 

<head> 
    <meta charset="UTF-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Roger Wang">
    
    <meta name="description" content="Time Series Forecasting of Crime Rates in San Francisco.">
    <title></title> 

    <link id="stylesheet" rel="stylesheet" href="../stylesheets/posts.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Oswald|Quattrocento+Sans">
    <script src="script_post.js" defer></script>
</head>

<body>
    <nav>
        <ul>
            <li><a href="../index.html">Portfolio</a></li>
            <li id="current-page"><a href="../blog.html">Blog</a></li>
            <li><a href="../contact.html">Contact</a></li>
        </ul>
     </nav>

     <main>
        <h1 id="title">Forecasting Daily Crime Rates in San Francisco using XGBoost</h1>
        <div class="figure">
            <img alt="Alcatraz" src="../imgs/3/alcatraz.jpg">
            <p>Alcatraz - a reflection of San Francisco's rugged history.</p>
        </div>
        <p class="paragraph">
            Every city has its own rhythm, a heartbeat shaped by the people, streets, and the seasons. 
            In San Francisco, this pulse is reflected through its crime rates and police reports.
        </p>
        <p class="paragraph">
            What if we could forecast that rhythm, day by day?
        </p>
        <p class="paragraph">
            Using over 900,000 police reports and XGBoost, this project builds a time series model to predict 
            daily crime rates for the year 2025. From handling messy data to fine-tuning model predictions, 
            heres how I approached the challenge.
        </p>

        <p class="heading">San Francisco's Police Reports Dataset</p>
        <p class="paragraph">
            This project uses a dataset of over 900,000 police incident reports in San Francisco from 
            January 2018 to March 2025. The data is publicaly available through <a href="https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data">the DataSF initiative,</a> which 
            makes city data accessible for public use and transparency.
        </p>
        <p class="paragraph">
            Each row is an incident report containing details such as location, time, and type of offense. 
            A complete anaylsis of this dataset, including how I unified incident categories, is presented in 
            <a href="https://rogerwng.github.io/posts/2_EDA_SFCrimes.html">my exploratory data analysis post.</a> Unifying categories is important to separate criminal and 
            non-criminal incidents, as this project focuses on forecasting crime and other major offenses.
        </p>
        <p class="paragraph">
            The time series data is created by aggregating each timestep to the nearest day, resulting in the daily crime rates shown below:
        </p>

        <div class="figure">
            <img alt="Raw Time Series Data" src="../imgs/3/rawtimeseries.png">
            <p>Figure 1. Raw time series data.</p>
        </div>

        <p class="paragraph">
            The forecasting task is to predict the crime rate of the next day based on the historical data.
        </p>

        <p class="heading">Feature Engineering</p>
        <p class="paragraph">
            First I addressed outliers, which can distort model accuracy, convergence, and evaluation metrics such as RMSE, which is especially sensitive 
            to large residuals. I compared two methods of outlier handling: rolling window winsorization and 
            rolling z-score replacement. 
        </p>
        <p class="paragraph">
            Winsorizing replaces extreme datapoints with values at the percentile cutoffs. 
            This means any datapoint outside of the cutoff range is replaced by the nearest bound. For example, 
            in a 90-day window and cutoffs at the 1st/99th percentiles, the points greater than the 99th percentile 
            would be replaced by the value at the 99th percentile, and vice versa. Here's how this looks:
        </p>

        <div class="figure">
            <img alt="Data After Winsorization" src="../imgs/3/winsorizing.png">
            <p>Figure 2. Time series after winsorization.</p>
        </div>

        <p class="paragraph">
            Rolling z-score replacement calculates the mean and standard deviation. Z-score is 
            the difference between a datapoint and the mean in units of standard deviation. All points outside a 
            threshold are clipped to fit within that range. Here is what the data looks like under a threshold of 2 
            standard deviations over a 30-day window:
        </p>

        <div class="figure">
            <img alt="Data After Z-Score Replacement" src="../imgs/3/zscore.png">
            <p>Figure 3. Time series after z-score replacement.</p>
        </div>

        <p class="paragraph">
            I ultimately chose rolling z-scores as it provided a visually smoother dataset. (Although models were still 
            trained on both original and zscore data).
        </p>
        <p class="paragraph">
            A first look at the time series reveals clear yearly seasonality, with crime rates dipping early in the year 
            and rising throughout the summer. Differencing and PACF plots show additional structure, including strong 
            autocorrelations up to a 7-day lag. Based on these insights, I created lag features for the previous 1, 3, and 5 days.
        </p>

        <div class="figure">
            <img alt="Differenced Data and PACF" src="../imgs/3/rawdifferencepacf.png">
            <p>Figure 4. Differenced data and PACF plots.</p>
        </div>

        <p class="paragraph">
            To capture broader trends, I also computed rolling window statistics over 3, 7, 15, and 30-day windows. These include: 
            mean, standard deviation, and trend (difference between successive window means).
        </p>
        <p class="paragraph">
            All features were calculated carefully to prevent data leakage by withholding any information beyond the 
            current date. Since the target variable is the next day's crime rates using current and past information, 
            this approach is safe.
        </p>
        <p class="paragraph">
            Additionally, I extracted several calendar-based features. These include year, month, day of the week, 
            day of the month, day of the year, and boolean indicators for weekends and holidays.
        </p>
        <p class="paragraph">
            To preserve the cyclical nature of time features, I encoded month, day of the week, day of the month, 
            and day of the year using sin and cos transformations.
        </p>
        <p class="paragraph">
            The final time series dataset has 32 predictor features and 1 target, the crime rate of the next day. 
        </p>

        <p class="heading">Modelling with XGBoost</p>
        <p class="paragraph">
            With the time series data now set up as a standard supervised learning problem, we can apply XGBoost, a powerful gradient
            boosting library well-suited for tabular datasets. While traditional time series 
            models like ARIMA or Exponential Smoothing, and deep learning approaches such as LSTMs or Transformers are also 
            common, XGBoost offers strong performance with less tuning and greater interpretability.
        </p>
        <p class="paragraph">
            The key in training XGBoost as a time series forecaster is to avoid 
            data leakage. Instead of randomizing the train/test split we instead 
            split the data chronologically at a specific cutoff date.
        </p>

        <div class="figure">
            <img alt="Train/Test Split" src="../imgs/3/traintestsplit.png">
            <p>Figure 5. A simple train/test split.</p>
        </div>
        
        <p class="paragraph">
            The choice of cutoff date depends on the forecasting horizon, how far ahead we want to predict. Since we 
            want to forecast the remainder of 2025, from March to December, we set a test size of at least 9 months. 
        </p>
        <p class="paragraph">
            While a single train/test split is good for a baseline, we need a more robust strategy for fine-tuning. 
            Thus, we apply time series cross-validation. We select a cutoff date for each fold, use a defined 
            window of data after that date for validation, and advance the cutoff for the next fold, averaging the scores as we go.
        </p>

        <div class="figure">
            <img alt="Cross-validation Splits with 3 Folds" src="../imgs/3/kfolds.png">
            <p>Figure 6. Cross-validation split with 3 folds.</p>
        </div>

        <p class="paragraph">
            Using this strategy, we can fine-tune our XGBoost model via Optuna and score each hyperparameter state with our time series cross-validation. 
        </p>
        <p class="paragraph">
            Finally, the full model is trained on the entire dataset with the best hyperparameters. 
        </p>
        <p class="paragraph">
            To generate future predictions, we use recursive forecasting. The model predicts the next day's crime rate using the latest data. Then, 
            the features are computed for the next step. We repeat this process throughout our forecast horizon.
        </p>
        <p class="paragraph">
            Thus, the model forecasts the future one day at a time by continuously updating the features with its own predictions.
        </p>

        <div class="figure">
            <img alt="Block Diagram of Data and Model Pipeline" src="../imgs/3/sfcrime_forecast_block.png">
            <p>Figure 7. The data and model pipeline.</p>
        </div>

        <p class="heading">Results</p>
        <p class="paragraph">
            After fine-tuning our model for 1000 trials, we get a best RMSE of 24.89. Here is how well the model fits the training data, at the macro and micro levels:
        </p>

        <div class="figure">
            <img alt="Fine-tuned Model Performance on Dataset" src="../imgs/3/trainseteval.png">
            <p>Figure 8. Performance of best model on dataset.</p>
        </div>

        <p class="paragraph">
            Note that our model evaluations don't fully reflect their forecasting ability. After all, the cross-validation 
            scores are taken by using the model to predict on the entire test set at once, given no errors in features like rolling means. Recursive prediction on the other hand,
            tendd to accumulate errors over time. This makes long-range forecasting more challenging and sensitive.
        </p>
        <p class="paragraph">
            Here is a visualization of how this model performs on each cross-validation split.
        </p>

        <div class="figure">
            <img alt="Fine-tune Model Performance on Cross-Validation" src="../imgs/3/kfoldforecast.png">
            <p>Figure 9. Performance of best model on each CV fold.</p>
        </div>

        <p class="paragraph">
            Finally, here is our model's prediction of crime rates in 2025.
        </p>

        <div class="figure">
            <img alt="2025 Crime Rate Forecast" src="../imgs/3/2025forecast.png">
            <p>Figure 10. Crime rate forecast for 2025.</p>
        </div>

        <p class="heading">Future Directions</p>
        <p class="paragraph">
            Although our model has decent performance there is a lot of room for improvement. 
        </p>
        <p class="paragraph">
            Currently, the model produces a point estimate, a single value that forecasts the crime rate for the next day. 
            Point forecasts lack robustness in noisy real-world settings. Predicting a range of likely values with some level of uncertainty
            would be a more informative appraoch.
        </p>
        <p class="paragraph">
            This is where quantile regression becomes valuable. XGBoost supports quantile objectives, which allow the training 
            of separate models for the lower and upper bounds (10th and 90th percentiles for example) of a forecast. These define 
            a confidence interval, a probabilisic region where the actual crime rate is likely to land. Future work on 
            this project could explore quantile regression and compare its forecasting performance.
        </p>
        <p class="paragraph">
            Additionally, it would be insightful to compare XGBoost with other popular time series 
            strategies. These include classicial methods such as ARIMA and Exponential Smoothing as well as modern deep 
            learning approaches like LSTMs and Transformers. 
        </p>
        <p class="paragraph">
            Evaluating various models could give insight about trade-offs between model complexity, interpretability, and accuracy. 
            Furthermore, the most effective methods for this type of forecasting problem can be identified.
        </p>

     </main>

     <footer>
        <p>Roger Wang - Published May 23rd, 2025 - Last Updated Never</p>
        <div class="references">
            <p>References</p>
            <ul>
                <li><a href="https://github.com/rogerwng/SFPD_Crime">[1] Github Project Repo</a></li>
                <li><a href="https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data">[2] Dataset from DataSF</a></li>
                <li><a href="https://rogerwng.github.io/posts/2_EDA_SFCrimes.html">[3] EDA Blog Post</a></li>
            </ul>
        </div>
     </footer>
</body>


</html>