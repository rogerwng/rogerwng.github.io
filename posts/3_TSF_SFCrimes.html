<!DOCTYPE html>
<html lang="en"> 

<head> 
    <meta charset="UTF-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Roger Wang">
    
    <meta name="description" content="Time Series Forecasting of Crime Rates in San Francisco.">
    <title></title> 

    <link id="stylesheet" rel="stylesheet" href="../stylesheets/posts.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Oswald|Quattrocento+Sans">
    <script src="script_post.js" defer></script>
</head>

<body>
    <nav>
        <ul>
            <li><a href="../index.html">Portfolio</a></li>
            <li id="current-page"><a href="../blog.html">Blog</a></li>
            <li><a href="../contact.html">Contact</a></li>
        </ul>
     </nav>

     <main>
        <h1 id="title">Forecasting Daily Crime Rates in San Francisco using XGBoost</h1>
        <div class="figure">
            <img alt="" src="">
            <p></p>
        </div>
        <p class="paragraph">
            Every city has its own rhythm, a heartbeat shaped by the people, streets, and the seasons. 
            In San Francisco, this pulse is reflected through its crime rates and police reports.
        </p>
        <p class="paragraph">
            What if we could forecast that rhythm, day by day?
        </p>
        <p class="paragraph">
            Using over 900,000 police reports and XGBoost, this project builds a time series model to predict 
            daily crime rates for the year 2025. From handling messy data to fine-tuning model predictions, 
            heres how I approached the challenge.
        </p>

        <p class="heading">San Francisco's Police Reports Dataset</p>
        <p class="paragraph">
            This project uses a dataset of over 900,000 police incident reports in San Francisco from 
            January 2018 to March 2025. The data is publicaly available through <a href="https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data">the DataSF initiative,</a> which 
            makes city data accessible for public use and transparency.
        </p>
        <p class="paragraph">
            Each row is an incident report containing details such as location, time, and type of offense. 
            A complete anaylsis of this dataset, including how I unified incident categories, is presented in 
            <a href="https://rogerwng.github.io/posts/2_EDA_SFCrimes.html">my exploratory data analysis post.</a> Unifying categories is important to separate criminal and 
            non-criminal incidents, as this project focuses on forecasting crime and other major offenses.
        </p>
        <p class="paragraph">
            The time series data is created by rounding each timestep to its nearest day, and then aggregating 
            the daily data. This results in daily crime rates from January 1st, 2018 to March 2nd, 2025, 
            a total of 2618 datapoints. The raw data is shown below:
        </p>

        <div class="figure">
            <img alt="Raw Time Series Data" src="../imgs/3/rawtimeseries.png">
            <p>Raw time series data</p>
        </div>

        <p class="paragraph">
            Although using hourly data would provide more datapoints, the resulting time series was extremely 
            noisy and lacked any clear trends or patterns. In contrast, daily crime rates show cleaner structure and 
            is more suitable for forecasting.
        </p>
        <p class="paragraph">
            The forecasting task is to predict the crime rate of the next day based on the historical data.
        </p>

        <p class="heading">Feature Engineering</p>
        <p class="paragraph">
            First I addressed outliers, of which this dataset has many. Outliers can distort model accuracy 
            and negatively impact convergence and evaluation metrics such as RMSE, which is especially sensitive 
            to large residuals. I compared two methods of outlier handling: rolling window winsorization and 
            rolling z-score replacement. 
        </p>
        <p class="paragraph">
            Winsorizing over a window replaces extreme datapoints with values at the percentile cutoffs. 
            This means any datapoint outside of the cutoff range are replaced by their nearest bounds. For example, 
            in a 90-day window and cutoffs at the 1st/99th percentiles, the points greater than the 99th percentile 
            would be replaced by the value at that upper bound, and vice versa. Here's how the data looks after this transformation:
        </p>

        <div class="figure">
            <img alt="Data After Winsorization" src="../imgs/3/winsorizing.png">
            <p>Time series after winsorization</p>
        </div>

        <p class="paragraph">
            Rolling z-score replacement calculates the mean and standard deviation of the window. Z-score is calculated 
            as the difference between a datapoint and the mean in units of standard deviation. All points outside the 
            threshold are clipped to fit within that range. Here is what the data looks like under a threshold of 2 
            standard deviations over a 30-day window:
        </p>

        <div class="figure">
            <img alt="Data After Z-Score Replacement" src="../imgs/3/zscore.png">
            <p>Time series after z-score replacement</p>
        </div>

        <p class="paragraph">
            I ultimately chose rolling z-scores as it provided a visually smoother dataset. (Although models were still 
            trained on both original and zscore data).
        </p>
        <p class="paragraph">
            A first look at the time series reveals clear yearly seasonality, with crime rates dipping early in the year 
            and rising throughout the summer. Differencing and PACF plots show additional structure, including strong 
            autocorrelations up to a 7-day lag. Based on these insights, I created lag features for the previous 1, 3, and 5 days.
        </p>

        <div class="figure">
            <img alt="Differenced Data and PACF" src="../imgs/3/rawdifferencepacf.png">
            <p>Differenced data and PACF plots</p>
        </div>

        <p class="paragraph">
            To capture broader trends, I also computed rolling window statistics over 3, 7, 15, and 30-day windows. These include: 
            mean, standard deviation, and trend (difference between successive window means).
        </p>
        <p class="paragraph">
            All features were calculated carefully to prevent data leakage, without including any information beyond the 
            current date. Since the target variable is the next day's crime rates using current and past information, 
            this approach is safe.
        </p>
        <p class="paragraph">
            Additionally, I extracted several calendar-based features. These include year, month, day of the week, 
            day of the month, and day of the year. I also added boolean indicators for weekends and holidays.
        </p>
        <p class="paragraph">
            To preserve the cyclical nature of time features, I encoded month, day of the week, day of the month, 
            and day of the year using sin and cos transformations.
        </p>
        <p class="paragraph">
            After these efforts, the final time series dataset has 32 predictor features and 1 target, the crime rate of the next day. 
        </p>

        <p class="heading">Modelling with XGBoost</p>
        <p class="paragraph">
            With the time series data now set up as a standard supervised learning problem, we can apply XGBoost. 
            This is a powerful gradient boosting library well-suited for tabular datasets. While traditional time series 
            models like ARIMA or Exponential Smoothing, and deep learning approaches such as LSTMs or Transformers are also 
            common, XGBoost offers strong performance with less tuning and greater interpretability.
        </p>
        <p class="paragraph">
            The key in training XGBoost as a time series forecaster is to take care in our train/test split to avoid 
            data leakage. Instead of randomizing the train/test split as we would in traditional problems, we instead 
            split the data chronologically at a specific cutoff date.
        </p>

        <div class="figure">
            <img alt="Train/Test Split" src="../imgs/3/traintestsplit.png">
            <p>A simple train/test split</p>
        </div>
        
        <p class="paragraph">
            The choice of cutoff date depends on the forecasting horizon, how far ahead we want to predict. Since we 
            want to forecast the remainder of 2025, 9 months from March to December, we set a test size of at least 9 months. 
        </p>
        <p class="paragraph">
            While a single train/test split is good for a baseline, we need a more robust strategy for fine-tuning. 
            To do this, we apply time series cross-validation. We select a cutoff date for each fold, use a defined 
            window of data after that date for validation, and advance the cutoff for the next fold. We average our 
            scores as we go.
        </p>

        <div class="figure">
            <img alt="Cross-validation Splits with 3 Folds" src="">
            <p>Cross-validation split with 3 folds</p>
        </div>

        <p class="paragraph">
            Using this strategy, we can fine-tune our XGBoost model via Optuna and score each hyperparameter state with our time series cross-validation. 
        </p>
        <p class="paragraph">
            Finally, the full model is trained on the entire dataset with the best hyperparameters. 
        </p>
        <p class="paragraph">
            To generate future predictions, we use recursive forecasting. The model predicts the next day's crime rate using the latest data. Then, 
            the features are computed for the next step. We repeat this process throughout our forecast horizon.
        </p>
        <p class="paragraph">
            Thus, the model forecasts the future one day at a time by continuously updating the features with its own predictions.
        </p>

        <div class="figure">
            <img alt="Block Diagram of Data and Model Pipeline" src="">
            <p>The data and model pipeline</p>
        </div>

        <p class="heading">Results</p>
        <p class="paragraph">
            After fine-tuning our model for 1000 trials, we get a best RMSE of 24.89. Here is how well the model fits the training data, at the macro and micro levels:
        </p>

        <div class="figure">
            <img alt="Fine-tuned Model Performance on Dataset" src="../imgs/3/trainseteval.png">
            <p>Performance of best model on dataset</p>
        </div>

        <p class="paragraph">
            Note that our model evaluations don't fully reflect their forecasting ability. After all, the cross-validation 
            scores are taken by using the model to predict on the entire test set at once, assuming all past information is 
            known. Recursive prediction on the other hand, means that any errors that the model makes in predictions 
            eventually stack up and compound over time. This makes long-range forecasting more challenging and sensitive to errors.
        </p>
        <p class="paragraph">
            Here is a visualization of how this model performs on each cross-validation split (separate model at each split, same hyperparams).
        </p>

        <div class="figure">
            <img alt="Fine-tune Model Performance on Cross-Validation" src="../imgs/3/kfoldforecast.png">
            <p>Performance of best model on each CV fold</p>
        </div>

        <p class="paragraph">
            Finally, here is our model's prediction of crime rates in 2025.
        </p>

        <div class="figure">
            <img alt="2025 Crime Rate Forecast" src="../imgs/3/2025forecast.png">
            <p>Crime rate forecast for 2025</p>
        </div>

        <p class="heading">Future Directions</p>
        <p class="paragraph">
            Although our model has decent performance there is a lot of room for improvement. 
        </p>
        <p class="paragraph">
            Currently, the model produces a point estimate, a single value that forecasts the crime rate for the next day. 
            Point forecasts usually lack robustness, especially in noisy real-world settings. A better approach would be 
            to predict a range of likely values thereby introducing a level of forecast uncertainty.
        </p>
        <p class="paragraph">
            This is where quantile regression becomes valuable. XGBoost supports quantile objectioves, which allow the training 
            of separate models for the lower and upper bounds (10th and 90th percentiles for example). These bounds define 
            a confidence interval, a probabilisic region where the actual crime rate is likely to land. Future work on 
            this project could explore quantile regression and compare its forecasting performance against point-based models.
        </p>
        <p class="paragraph">
            Additionally, while XGBoost is a great baseline, it would be insightful to compare it with other popular time series 
            strategies. These include classicial methods such as ARIMA and Exponential Smoothing as well as modern deep 
            learning approaches like LSTMs and Transformers. 
        </p>
        <p class="paragraph">
            Evaluating various models could give insight about trade-offs between model complexity, interpretability, and accuracy. 
            Furthermore, the most effective methods for this type of forecasting problem can be identified.
        </p>

     </main>

     <footer>
        <p>Roger Wang - Published May 23rd, 2025 - Last Updated Never</p>
        <div class="references">
            <p>References</p>
            <ul>
                <li><a href="https://github.com/rogerwng/SFPD_Crime">[1] Github Project Repo</a></li>
                <li><a href="https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data">[2] Dataset from DataSF</a></li>
                <li><a href="https://rogerwng.github.io/posts/2_EDA_SFCrimes.html">[3] EDA Blog Post</a></li>
            </ul>
        </div>
     </footer>
</body>


</html>